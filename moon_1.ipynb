{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "import albumentations as A\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# Configuration \n",
    "CONFIG = {\n",
    "    'data_dir': \"D:\\\\Work\\\\dont_plot_images\\\\IPI App Data-20250119T224817Z-001\\\\IPI App Data\",\n",
    "    'augmented_data_dir': \"D:\\\\Work\\\\dont_plot_images\\\\augmented_data\", \n",
    "    'batch_size': 8,      # Small batch size to prevent OOM\n",
    "    'num_epochs': 150,\n",
    "    'learning_rate': 0.0002,\n",
    "    'weight_decay': 0.005,\n",
    "    'image_size': 224,    # Standard ImageNet size\n",
    "    'train_ratio': 0.7,\n",
    "    'val_ratio': 0.15,\n",
    "    'test_ratio': 0.15,\n",
    "    'num_workers': 2      # Reduced workers to limit memory usage\n",
    "}\n",
    "\n",
    "# Device configuration - keep it simple\n",
    "device = torch.device(\"cpu\")  # Default to CPU to prevent OOM errors\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Memory cleanup function\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "class SimplifiedAugmentation:\n",
    "    def __init__(self, num_variations=3):  # Reduced from 5 to 3\n",
    "        self.num_variations = num_variations\n",
    "        self.transform_sets = [\n",
    "            # Set 1: Basic Transformations\n",
    "            A.Compose([\n",
    "                A.RandomRotate90(p=0.7),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.RandomBrightnessContrast(p=0.7),\n",
    "            ]),\n",
    "            \n",
    "            # Set 2: Geometric Transformations\n",
    "            A.Compose([\n",
    "                A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=45, p=0.7),\n",
    "            ]),\n",
    "            \n",
    "            # Set 3: Mixed Transformations\n",
    "            A.Compose([\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.RandomBrightnessContrast(p=0.5),\n",
    "            ])\n",
    "        ]\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        augmented_images = []\n",
    "        img_array = np.array(img)\n",
    "        \n",
    "        if len(img_array.shape) == 2:\n",
    "            img_array = np.stack([img_array] * 3, axis=-1)\n",
    "            \n",
    "        # Original image\n",
    "        augmented_images.append(img)\n",
    "        \n",
    "        # Generate variations using different transform sets\n",
    "        for transform_set in self.transform_sets:\n",
    "            augmented = transform_set(image=img_array)['image']\n",
    "            augmented_images.append(Image.fromarray(augmented))\n",
    "            \n",
    "        return augmented_images\n",
    "\n",
    "def create_augmented_dataset():\n",
    "    \"\"\"Simplified augmentation pipeline\"\"\"\n",
    "    original_data_dir = CONFIG['data_dir']\n",
    "    augmented_data_dir = CONFIG['augmented_data_dir']\n",
    "    \n",
    "    if not os.path.exists(augmented_data_dir):\n",
    "        os.makedirs(augmented_data_dir)\n",
    "    \n",
    "    # Get all class folders\n",
    "    class_folders = []\n",
    "    \n",
    "    # Process training directory\n",
    "    if os.path.exists(os.path.join(original_data_dir, 'train')):\n",
    "        for class_name in os.listdir(os.path.join(original_data_dir, 'train')):\n",
    "            class_path = os.path.join(original_data_dir, 'train', class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                class_folders.append((class_path, class_name))\n",
    "    \n",
    "    # Process test directory\n",
    "    if os.path.exists(os.path.join(original_data_dir, 'test')):\n",
    "        for class_name in os.listdir(os.path.join(original_data_dir, 'test')):\n",
    "            class_path = os.path.join(original_data_dir, 'test', class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                class_folders.append((class_path, class_name))\n",
    "    \n",
    "    # Track all images\n",
    "    all_images = []\n",
    "    \n",
    "    # Create augmentation pipeline\n",
    "    augmentation = SimplifiedAugmentation(num_variations=3)\n",
    "    \n",
    "    # Augment all images\n",
    "    for class_path, class_name in class_folders:\n",
    "        # Create class directory\n",
    "        augmented_class_dir = os.path.join(augmented_data_dir, class_name)\n",
    "        if not os.path.exists(augmented_class_dir):\n",
    "            os.makedirs(augmented_class_dir)\n",
    "        \n",
    "        # Process images\n",
    "        for img_name in tqdm(os.listdir(class_path), desc=f\"Augmenting {class_name}\"):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            \n",
    "            if not img_path.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.webp')):\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                augmented_images = augmentation(img)\n",
    "                \n",
    "                for i, aug_img in enumerate(augmented_images):\n",
    "                    suffix = f\"_aug{i}\" if i > 0 else \"\"\n",
    "                    base_name, ext = os.path.splitext(img_name)\n",
    "                    aug_img_path = os.path.join(augmented_class_dir, f\"{base_name}{suffix}{ext}\")\n",
    "                    aug_img.save(aug_img_path)\n",
    "                    all_images.append((aug_img_path, class_name))\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}\")\n",
    "    \n",
    "    clear_memory()\n",
    "    return all_images\n",
    "\n",
    "def split_dataset(all_images):\n",
    "    \"\"\"Split images into train/val/test sets\"\"\"\n",
    "    # Group by original image to prevent data leakage\n",
    "    image_groups = {}\n",
    "    for img_path, class_name in all_images:\n",
    "        # Extract base name without augmentation suffix\n",
    "        base_name = os.path.basename(img_path)\n",
    "        if \"_aug\" in base_name:\n",
    "            base_name = base_name.split(\"_aug\")[0]\n",
    "        \n",
    "        if base_name not in image_groups:\n",
    "            image_groups[base_name] = []\n",
    "        \n",
    "        image_groups[base_name].append((img_path, class_name))\n",
    "    \n",
    "    # Get unique image groups\n",
    "    unique_images = list(image_groups.keys())\n",
    "    \n",
    "    # Split into train, validation, and test sets\n",
    "    train_val_images, test_images = train_test_split(\n",
    "        unique_images, \n",
    "        test_size=CONFIG['test_ratio'], \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    train_images, val_images = train_test_split(\n",
    "        train_val_images, \n",
    "        test_size=CONFIG['val_ratio'] / (CONFIG['train_ratio'] + CONFIG['val_ratio']), \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create the split datasets\n",
    "    splits = {\n",
    "        'train': [],\n",
    "        'val': [],\n",
    "        'test': []\n",
    "    }\n",
    "    \n",
    "    # Add all variations of each original image to the appropriate split\n",
    "    for base_name in train_images:\n",
    "        splits['train'].extend(image_groups[base_name])\n",
    "    \n",
    "    for base_name in val_images:\n",
    "        splits['val'].extend(image_groups[base_name])\n",
    "    \n",
    "    for base_name in test_images:\n",
    "        splits['test'].extend(image_groups[base_name])\n",
    "    \n",
    "    # Create directories for the splits\n",
    "    for split in splits:\n",
    "        split_dir = os.path.join(CONFIG['augmented_data_dir'], split)\n",
    "        if not os.path.exists(split_dir):\n",
    "            os.makedirs(split_dir)\n",
    "        \n",
    "        # Create class directories\n",
    "        for _, class_name in splits[split]:\n",
    "            class_dir = os.path.join(split_dir, class_name)\n",
    "            if not os.path.exists(class_dir):\n",
    "                os.makedirs(class_dir)\n",
    "    \n",
    "    # Copy images to appropriate directories\n",
    "    for split, images in splits.items():\n",
    "        for img_path, class_name in tqdm(images, desc=f\"Copying {split} images\"):\n",
    "            dest_path = os.path.join(\n",
    "                CONFIG['augmented_data_dir'], \n",
    "                split, \n",
    "                class_name, \n",
    "                os.path.basename(img_path)\n",
    "            )\n",
    "            shutil.copy(img_path, dest_path)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\nDataset split complete:\")\n",
    "    print(f\"  Training: {len(splits['train'])} images\")\n",
    "    print(f\"  Validation: {len(splits['val'])} images\")\n",
    "    print(f\"  Test: {len(splits['test'])} images\")\n",
    "    \n",
    "    clear_memory()\n",
    "    return splits\n",
    "\n",
    "# Data transforms\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load datasets and create dataloaders\"\"\"\n",
    "    try:\n",
    "        # Create ImageFolder datasets\n",
    "        datasets_dict = {\n",
    "            split: torchvision.datasets.ImageFolder(\n",
    "                os.path.join(CONFIG['augmented_data_dir'], split),\n",
    "                transform=data_transforms[split]\n",
    "            )\n",
    "            for split in ['train', 'val', 'test']\n",
    "        }\n",
    "        \n",
    "        # Create dataloaders\n",
    "        dataloaders = {\n",
    "            split: torch.utils.data.DataLoader(\n",
    "                datasets_dict[split],\n",
    "                batch_size=CONFIG['batch_size'],\n",
    "                shuffle=(split == 'train'),\n",
    "                num_workers=CONFIG['num_workers'],\n",
    "                pin_memory=False\n",
    "            )\n",
    "            for split in ['train', 'val', 'test']\n",
    "        }\n",
    "        \n",
    "        # Get dataset sizes\n",
    "        dataset_sizes = {split: len(datasets_dict[split]) for split in ['train', 'val', 'test']}\n",
    "        \n",
    "        # Get class names\n",
    "        class_names = datasets_dict['train'].classes\n",
    "        \n",
    "        return dataloaders, dataset_sizes, class_names\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def create_model(num_classes):\n",
    "    \"\"\"Create a simplified ResNet18 model (smaller than ResNet50)\"\"\"\n",
    "    # ResNet18 is much smaller than ResNet50\n",
    "    model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "    \n",
    "    # Modify the classifier\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, dataloaders, dataset_sizes, num_epochs=25):\n",
    "    \"\"\"Training function with early stopping\"\"\"\n",
    "    since = time.time()\n",
    "    stats = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'test_loss': None,\n",
    "        'test_acc': None\n",
    "    }\n",
    "    \n",
    "    patience = 5  # Reduced patience for faster stopping\n",
    "    early_stopping_counter = 0\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Set up mixed precision if available\n",
    "    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "    \n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'\\nEpoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Only train and validate during training loop\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    clear_memory()\n",
    "                    model.train()\n",
    "                else:\n",
    "                    model.eval()\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                pbar = tqdm(dataloaders[phase], desc=f'{phase} epoch {epoch}')\n",
    "                \n",
    "                for inputs, labels in pbar:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # Forward pass\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # Backward + optimize only in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # Statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                    # Update progress bar\n",
    "                    batch_size = inputs.size(0)\n",
    "                    current_loss_avg = running_loss / ((pbar.n + 1) * batch_size)\n",
    "                    current_acc = running_corrects.double() / ((pbar.n + 1) * batch_size)\n",
    "                    pbar.set_postfix({\n",
    "                        'loss': f'{current_loss_avg:.4f}',\n",
    "                        'acc': f'{current_acc:.4f}'\n",
    "                    })\n",
    "\n",
    "                    # Free memory\n",
    "                    del inputs, labels, outputs\n",
    "                    clear_memory()\n",
    "\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "                if phase == 'train':\n",
    "                    stats['train_loss'].append(epoch_loss)\n",
    "                    stats['train_acc'].append(epoch_acc.item())\n",
    "                else:\n",
    "                    stats['val_loss'].append(epoch_loss)\n",
    "                    stats['val_acc'].append(epoch_acc.item())\n",
    "\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "                # Early stopping logic\n",
    "                if phase == 'val':\n",
    "                    if epoch_loss < best_val_loss:\n",
    "                        best_val_loss = epoch_loss\n",
    "                        early_stopping_counter = 0\n",
    "                        torch.save(model.state_dict(), best_model_params_path)\n",
    "                    else:\n",
    "                        early_stopping_counter += 1\n",
    "                    \n",
    "                    if early_stopping_counter >= patience:\n",
    "                        print(f'Early stopping triggered at epoch {epoch}')\n",
    "                        model.load_state_dict(torch.load(best_model_params_path))\n",
    "                        # Test on the test set before returning\n",
    "                        test_loss, test_acc = evaluate_model(model, criterion, dataloaders['test'], dataset_sizes['test'])\n",
    "                        stats['test_loss'] = test_loss\n",
    "                        stats['test_acc'] = test_acc\n",
    "                        return model, stats\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "        \n",
    "        # Test on the test set after training\n",
    "        test_loss, test_acc = evaluate_model(model, criterion, dataloaders['test'], dataset_sizes['test'])\n",
    "        stats['test_loss'] = test_loss\n",
    "        stats['test_acc'] = test_acc\n",
    "        \n",
    "    clear_memory()\n",
    "    return model, stats\n",
    "\n",
    "def evaluate_model(model, criterion, dataloader, dataset_size):\n",
    "    \"\"\"Evaluate model on a dataset\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            # Free memory\n",
    "            del inputs, labels, outputs\n",
    "            clear_memory()\n",
    "    \n",
    "    loss = running_loss / dataset_size\n",
    "    acc = running_corrects.double() / dataset_size\n",
    "    \n",
    "    print(f'Test Loss: {loss:.4f} Acc: {acc:.4f}')\n",
    "    \n",
    "    return loss, acc.item()\n",
    "\n",
    "def visualize_model(model, dataloaders, class_names, num_images=6):\n",
    "    \"\"\"Visualize model predictions\"\"\"\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['test']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            for j in range(min(num_images, inputs.size()[0])):\n",
    "                plt.subplot(2, num_images//2, j+1)\n",
    "                plt.axis('off')\n",
    "                plt.title(f'predicted: {class_names[preds[j]]}')\n",
    "                img = inputs.cpu().data[j].numpy().transpose((1, 2, 0))\n",
    "                mean = np.array([0.485, 0.456, 0.406])\n",
    "                std = np.array([0.229, 0.224, 0.225])\n",
    "                img = std * img + mean\n",
    "                img = np.clip(img, 0, 1)\n",
    "                plt.imshow(img)\n",
    "            plt.tight_layout()\n",
    "            break\n",
    "    \n",
    "    model.train(mode=was_training)\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        print(\"Step 1: Creating augmented dataset...\")\n",
    "        if not os.path.exists(CONFIG['augmented_data_dir']) or len(os.listdir(CONFIG['augmented_data_dir'])) == 0:\n",
    "            all_images = create_augmented_dataset()\n",
    "            print(f\"Created {len(all_images)} augmented images\")\n",
    "            \n",
    "            print(\"Step 2: Splitting dataset into train, validation, and test sets...\")\n",
    "            splits = split_dataset(all_images)\n",
    "        else:\n",
    "            print(\"Using existing augmented dataset...\")\n",
    "        \n",
    "        print(\"Step 3: Loading data...\")\n",
    "        dataloaders, dataset_sizes, class_names = load_data()\n",
    "        \n",
    "        # Print dataset statistics\n",
    "        print(\"\\nDataset Statistics:\")\n",
    "        print(f\"Total training samples: {dataset_sizes['train']}\")\n",
    "        print(f\"Total validation samples: {dataset_sizes['val']}\")\n",
    "        print(f\"Total test samples: {dataset_sizes['test']}\")\n",
    "        print(f\"Number of classes: {len(class_names)}\")\n",
    "        \n",
    "        print(\"\\nStep 4: Initializing model...\")\n",
    "        model = create_model(len(class_names))\n",
    "        \n",
    "        # Try GPU, but with fallback to CPU if there's an issue\n",
    "        if torch.cuda.is_available():\n",
    "            try:\n",
    "                # Small test tensor to check if GPU has memory\n",
    "                test = torch.ones(1).cuda()\n",
    "                del test\n",
    "                # If we get here, GPU seems available\n",
    "                global device\n",
    "                device = torch.device(\"cuda:0\")\n",
    "                print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            except RuntimeError:\n",
    "                print(\"GPU memory issue detected, falling back to CPU\")\n",
    "                device = torch.device(\"cpu\")\n",
    "        \n",
    "        model = model.to(device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=CONFIG['learning_rate'],\n",
    "            weight_decay=CONFIG['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # Simple step LR scheduler\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "        \n",
    "        print(\"\\nStep 5: Starting training...\")\n",
    "        model, stats = train_model(\n",
    "            model, criterion, optimizer, scheduler,\n",
    "            dataloaders, dataset_sizes, num_epochs=CONFIG['num_epochs']\n",
    "        )\n",
    "        \n",
    "        print(\"\\nStep 6: Plotting results...\")\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(stats['train_loss'], label='Train')\n",
    "        plt.plot(stats['val_loss'], label='Validation')\n",
    "        plt.title('Loss vs. Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(stats['train_acc'], label='Train')\n",
    "        plt.plot(stats['val_acc'], label='Validation')\n",
    "        plt.title('Accuracy vs. Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_results.png')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nStep 7: Testing final model on test set...\")\n",
    "        test_loss, test_acc = stats['test_loss'], stats['test_acc']\n",
    "        print(f\"Final Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "        \n",
    "        print(\"\\nStep 8: Visualizing model predictions...\")\n",
    "        visualize_model(model, dataloaders, class_names)\n",
    "        plt.show()\n",
    "        \n",
    "        # Save the final model\n",
    "        torch.save({\n",
    "            'epoch': CONFIG['num_epochs'],\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'stats': stats,\n",
    "            'class_names': class_names\n",
    "        }, 'final_model.pth')\n",
    "        \n",
    "        print(\"Training pipeline complete!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
